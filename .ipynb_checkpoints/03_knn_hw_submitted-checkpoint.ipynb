{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised learning introduction, K-Nearest Neighbors (KNN)\n",
    "-----\n",
    "\n",
    "\n",
    "#### 1. Read data\n",
    "\n",
    "The provided three files (glass.data, glass.tag, glass.names) contains a small dataset. \"[The study of classification of types of glass was motivated by criminological investigation. At the scene of the crime, the glass left can be used as evidence...if it is correctly identified!](https://archive.ics.uci.edu/ml/datasets/Glass+Identification)\"\n",
    "\n",
    " - read the content of the glass.data file as a pandas dataframe\n",
    " - use the column names found in the glass.names file\n",
    " - if needed, handle the missing values\n",
    " - get rid of the ID column\n",
    " - separate the GlassType from the dataset and store it in a one-hot encoded manner (if we have 3 classes, than instead of 1, 2, 3 we use [1, 0, 0], [0, 1, 0] and [0, 0, 1]. So each dimension represents a class and 1 means the sample belongs to that class, 0 is the opposite)\n",
    "    - be careful, the data contains only 1-2-3-5-6-7, 4 is missing! Convert 5 $\\to$ 4, 6 $\\to$ 5 and 7 $\\to$ 6 and then apply the one-got encoding\n",
    "    - also keep the converted labels, where y is 1-2-3-4-5-6, we will need them later\n",
    " \n",
    "-----\n",
    "\n",
    "#### 2. \\& 3. Implement KNN\n",
    "\n",
    "Implement the K-nearest neighbors regression algorithm using only pure Python3 and numpy! Use L2 distance to find the neighbors. The prediction for each class should be the number of neighbors supporing the given class divided by **k** (for example if **k** is 5 and we have 3 neighbors for class A, 2 for class B and 0 for class C neighbors, then the prediction for class A should be 3/5, for class B 2/5, for class C 0/5). \n",
    "\n",
    "Complete the function!\n",
    "\n",
    "```python\n",
    "def knn_classifier(k, X_train, y_train, X_test):\n",
    "    ...\n",
    "    return predictions\n",
    "```\n",
    " - **k** is the number of neighbors to be considered\n",
    " - **X_train** is the training data points\n",
    " - **X_test** is the test data points\n",
    " - **y_train** is the labels for the training data\n",
    " - assume that **y_test** is one-hot encoded.\n",
    "\n",
    "A valid-syntaxed input looks like:\n",
    "````python\n",
    "k = 2\n",
    "X_train = [[0.9, 0.2, 0.8] , [-1.2, 1.5, 0.7], [5.8, 0.0, 0.9], [6.2, 0.9, 0.9]]\n",
    "y_train = [[0, 1], [0, 1], [1, 0], [0, 1]]\n",
    "X_test  = [[0.8, 0.8, 0.6], [0.5, 0.4, 0.3]]\n",
    "```\n",
    "\n",
    "Here, it means that the training data consists of 4 points, each point is placed in a 3 dimensional space. And there are two possible classes for each point and there are two data points for that predictions is needed.\n",
    "\n",
    "#### 4. Predictions & interpretation\n",
    "\n",
    "- use every second (0, 2, 4, etc indicies) datapoint as training data and use the rest (1, 3, 5, ...) as test data\n",
    "- generate predictions with the implemented KNN with k=5\n",
    "- calculate the accuracy and the confusion matrix for the predictions\n",
    "- if the probability is the same for two or more classes select the first one from the left in the one-hot encoded version (or which has the smallest number in the original dataset)\n",
    "\n",
    "-----\n",
    "\n",
    "#### 5. Compare it to Sklearn's KNN\n",
    "- using the same train/test split generate predictions with sklearn KNNs. Use 5 neighbors again\n",
    "- are the predictions the same as for our implementation? (they should be)\n",
    "   - note: to get sklearn perform the same algorithm as out implementation it expects you to provide non one-hot encoding labels. That's why we kept them in the first exercise\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "\n",
    "### Hints:\n",
    " - On total you can get 10 points for fully completing all tasks.\n",
    " - Decorate your notebook with, questions, explanation etc, make it self contained and understandable!\n",
    " - Comments you code when necessary\n",
    " - Write functions for repetitive tasks!\n",
    " - Use the pandas package for data loading and handling\n",
    " - Use matplotlib and seaborn for plotting or bokeh and plotly for interactive investigation\n",
    " - Use the scikit learn package for almost everything\n",
    " - Use for loops only if it is really necessary!\n",
    " - Code sharing is not allowed between student! Sharing code will result in zero points.\n",
    " - If you use code found on web, it is OK, but, make its source clear! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Read data\n",
    "\n",
    "The provided three files (glass.data, glass.tag, glass.names) contains a small dataset. \"[The study of classification of types of glass was motivated by criminological investigation. At the scene of the crime, the glass left can be used as evidence...if it is correctly identified!](https://archive.ics.uci.edu/ml/datasets/Glass+Identification)\"\n",
    "\n",
    " - read the content of the glass.data file as a pandas dataframe\n",
    " - use the column names found in the glass.names file\n",
    " - if needed, handle the missing values\n",
    " - get rid of the ID column\n",
    " - separate the GlassType from the dataset and store it in a one-hot encoded manner (if we have 3 classes, than instead of 1, 2, 3 we use [1, 0, 0], [0, 1, 0] and [0, 0, 1]. So each dimension represents a class and 1 means the sample belongs to that class, 0 is the opposite)\n",
    "    - be careful, the data contains only 1-2-3-5-6-7, 4 is missing! Convert 5 $\\to$ 4, 6 $\\to$ 5 and 7 $\\to$ 6 and then apply the one-got encoding\n",
    "    - also keep the converted labels, where y is 1-2-3-4-5-6, we will need them later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from operator import add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['ID',\n",
    "                'RI',\n",
    "                'Na',\n",
    "                'Mg',\n",
    "                'Al',\n",
    "                'Si',\n",
    "                'K',\n",
    "                'Ca',\n",
    "                'Ba',\n",
    "                'Fe',\n",
    "                'Type_of_glass']\n",
    "\n",
    "df_glass = pd.read_csv('Data/glass.data',sep = ',',names = column_names)\n",
    "df_glass = df_glass.drop(['ID'],axis = 1)\n",
    "df_glass['Type_of_glass'].replace(to_replace=5,value = 4,inplace = True)\n",
    "df_glass['Type_of_glass'].replace(to_replace=6,value = 5,inplace = True)\n",
    "df_glass['Type_of_glass'].replace(to_replace=7,value = 6,inplace = True)\n",
    "\n",
    "def one_hot(value):\n",
    "    ret_list = 6*[0]\n",
    "    ret_list[value-1] = 1\n",
    "    return ret_list\n",
    "\n",
    "df_glass['Type_of_glass_hot'] = df_glass['Type_of_glass'].apply(lambda x: one_hot(x))\n",
    "\n",
    "\n",
    "glass_type = list(df_glass['Type_of_glass'])\n",
    "\n",
    "df_glass = df_glass.drop(['Type_of_glass'],axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RI</th>\n",
       "      <th>Na</th>\n",
       "      <th>Mg</th>\n",
       "      <th>Al</th>\n",
       "      <th>Si</th>\n",
       "      <th>K</th>\n",
       "      <th>Ca</th>\n",
       "      <th>Ba</th>\n",
       "      <th>Fe</th>\n",
       "      <th>Type_of_glass_hot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.52101</td>\n",
       "      <td>13.64</td>\n",
       "      <td>4.49</td>\n",
       "      <td>1.10</td>\n",
       "      <td>71.78</td>\n",
       "      <td>0.06</td>\n",
       "      <td>8.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.51761</td>\n",
       "      <td>13.89</td>\n",
       "      <td>3.60</td>\n",
       "      <td>1.36</td>\n",
       "      <td>72.73</td>\n",
       "      <td>0.48</td>\n",
       "      <td>7.83</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.51618</td>\n",
       "      <td>13.53</td>\n",
       "      <td>3.55</td>\n",
       "      <td>1.54</td>\n",
       "      <td>72.99</td>\n",
       "      <td>0.39</td>\n",
       "      <td>7.78</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.51766</td>\n",
       "      <td>13.21</td>\n",
       "      <td>3.69</td>\n",
       "      <td>1.29</td>\n",
       "      <td>72.61</td>\n",
       "      <td>0.57</td>\n",
       "      <td>8.22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.51742</td>\n",
       "      <td>13.27</td>\n",
       "      <td>3.62</td>\n",
       "      <td>1.24</td>\n",
       "      <td>73.08</td>\n",
       "      <td>0.55</td>\n",
       "      <td>8.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        RI     Na    Mg    Al     Si     K    Ca   Ba   Fe   Type_of_glass_hot\n",
       "0  1.52101  13.64  4.49  1.10  71.78  0.06  8.75  0.0  0.0  [1, 0, 0, 0, 0, 0]\n",
       "1  1.51761  13.89  3.60  1.36  72.73  0.48  7.83  0.0  0.0  [1, 0, 0, 0, 0, 0]\n",
       "2  1.51618  13.53  3.55  1.54  72.99  0.39  7.78  0.0  0.0  [1, 0, 0, 0, 0, 0]\n",
       "3  1.51766  13.21  3.69  1.29  72.61  0.57  8.22  0.0  0.0  [1, 0, 0, 0, 0, 0]\n",
       "4  1.51742  13.27  3.62  1.24  73.08  0.55  8.07  0.0  0.0  [1, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_glass.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RI                   0\n",
       "Na                   0\n",
       "Mg                   0\n",
       "Al                   0\n",
       "Si                   0\n",
       "K                    0\n",
       "Ca                   0\n",
       "Ba                   0\n",
       "Fe                   0\n",
       "Type_of_glass_hot    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_glass.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. \\& 3. Implement KNN\n",
    "\n",
    "Implement the K-nearest neighbors regression algorithm using only pure Python3 and numpy! Use L2 distance to find the neighbors. The prediction for each class should be the number of neighbors supporing the given class divided by **k** (for example if **k** is 5 and we have 3 neighbors for class A, 2 for class B and 0 for class C neighbors, then the prediction for class A should be 3/5, for class B 2/5, for class C 0/5). \n",
    "\n",
    "Complete the function!\n",
    "\n",
    "```python\n",
    "def knn_classifier(k, X_train, y_train, X_test):\n",
    "    ...\n",
    "    return predictions\n",
    "```\n",
    " - **k** is the number of neighbors to be considered\n",
    " - **X_train** is the training data points\n",
    " - **X_test** is the test data points\n",
    " - **y_train** is the labels for the training data\n",
    " - assume that **y_test** is one-hot encoded.\n",
    "\n",
    "A valid-syntaxed input looks like:\n",
    "````python\n",
    "k = 2\n",
    "X_train = [[0.9, 0.2, 0.8] , [-1.2, 1.5, 0.7], [5.8, 0.0, 0.9], [6.2, 0.9, 0.9]]\n",
    "y_train = [[0, 1], [0, 1], [1, 0], [0, 1]]\n",
    "X_test  = [[0.8, 0.8, 0.6], [0.5, 0.4, 0.3]]\n",
    "```\n",
    "\n",
    "Here, it means that the training data consists of 4 points, each point is placed in a 3 dimensional space. And there are two possible classes for each point and there are two data points for that predictions is needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 2\n",
    "X_train = [[0.9, 0.2, 0.8] , [-1.2, 1.5, 0.7], [5.8, 0.0, 0.9], [6.2, 0.9, 0.9]]\n",
    "y_train = [[0, 1], [0, 1], [1, 0], [0, 1]]\n",
    "X_test  = [[0.8, 0.8, 0.6], [0.5, 0.4, 0.3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_indexes(k,point,X_list):# gets the indexes of the k nearest neighbors\n",
    "    point_arr = np.array(point)\n",
    "    distances = [np.linalg.norm(point_arr - np.array(p)) for p in X_list]# L2 distance, works only on np.array\n",
    "    \n",
    "    return [distances.index(p) for p in sorted(distances)[:k]]\n",
    "\n",
    "def knn_predicted_value(k_indexes,y_train):# gets the predicted one-hot value\n",
    "    nearest_y = [y_train[i] for i in k_indexes]\n",
    "    pred = len(y_train[0])*[0]\n",
    "    \n",
    "    for n in nearest_y:\n",
    "        pred = list(map(add, pred,n))#sum two list element wise\n",
    "    \n",
    "    return [p/len(k_indexes) for p in pred] # division by k\n",
    "    \n",
    "\n",
    "def knn_classifier(k, X_train, y_train, X_test):\n",
    "    predictions = []\n",
    "    \n",
    "    for x_vals in X_test:\n",
    "        \n",
    "        x_vals_indexes = knn_indexes(k=k,point=x_vals,X_list=X_train)\n",
    "        predictions.append(knn_predicted_value(x_vals_indexes,y_train))\n",
    "    \n",
    "    \n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0, 1.0], [0.0, 1.0]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_classifier(k,X_train,y_train,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1]]\n",
      "[[0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=2)\n",
    "neigh.fit(X_train, y_train)\n",
    "print(neigh.predict([X_test[0]]))\n",
    "print(neigh.predict([X_test[1]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
    "# X = [[0], [1], [2], [3]]\n",
    "# >>> y = [0, 0, 1, 1]\n",
    "# >>> from sklearn.neighbors import KNeighborsClassifier\n",
    "# >>> neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "# >>> neigh.fit(X, y)\n",
    "# KNeighborsClassifier(...)\n",
    "# >>> print(neigh.predict([[1.1]]))\n",
    "# [0]\n",
    "# >>> print(neigh.predict_proba([[0.9]]))\n",
    "# [[0.66666667 0.33333333]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56.231663678038196"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1,4,5,6,7])\n",
    "b = np.array([2,4,61,2,4])\n",
    "np.linalg.norm(a-b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Conlusion 2-3\n",
    " The data is organized, and the my algorithm gives the same results as the sklearn one. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Predictions & interpretation\n",
    "\n",
    "- use every second (0, 2, 4, etc indicies) datapoint as training data and use the rest (1, 3, 5, ...) as test data\n",
    "- generate predictions with the implemented KNN with k=5\n",
    "- calculate the accuracy and the confusion matrix for the predictions\n",
    "- if the probability is the same for two or more classes select the first one from the left in the one-hot encoded version (or which has the smallest number in the original dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_maker(list_of_prob):\n",
    "    ret = len(list_of_prob)*[0]\n",
    "    ret[list(list_of_prob).index(max(list_of_prob))] = 1 #it selects the first occurence of max value, from left\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1,2,3,4,3,5].index(3)# selects the first occurence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "\n",
    "X_train = list(np.array(df_glass.iloc[::2][['RI','Na','Mg','Al','Si','K','Ca','Ba','Fe']]))\n",
    "y_train = list(df_glass.iloc[::2]['Type_of_glass_hot'])\n",
    "y_train_not_onehot = [y_train.index(y)+1 for y in y_train]\n",
    "\n",
    "X_test = list(np.array(df_glass.iloc[np.arange(1,len(df_glass),2)][['RI','Na','Mg','Al','Si','K','Ca','Ba','Fe']]))\n",
    "y_test = list(df_glass.iloc[np.arange(1,len(df_glass),2)]['Type_of_glass_hot'])\n",
    "y_test_not_onehot = [y.index(max(y))+1 for y in y_test]\n",
    "\n",
    "y_pred = knn_classifier(k,X_train,y_train,X_test)\n",
    "\n",
    "\n",
    "y_pred_one_hot = [one_hot_maker(y) for y in y_pred]\n",
    "y_pred_not_onehot = [y.index(max(y))+1 for y in y_pred_one_hot]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7009345794392523"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(np.array(y_test), np.array(y_pred_one_hot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[27,  7,  1,  0,  0,  0],\n",
       "       [ 5, 31,  1,  0,  1,  0],\n",
       "       [ 6,  0,  2,  0,  0,  0],\n",
       "       [ 0,  5,  0,  2,  0,  0],\n",
       "       [ 1,  1,  0,  0,  2,  0],\n",
       "       [ 1,  2,  1,  0,  0, 11]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test_not_onehot,y_pred_not_onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conlusion 4:\n",
    "The predictions are correct in ~70%. But there are some confusion between the classes. The 3rd and 4th classes less correct in proportion to the number of correct matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Compare it to Sklearn's KNN\n",
    "- using the same train/test split generate predictions with sklearn KNNs. Use 5 neighbors again\n",
    "- are the predictions the same as for our implementation? (they should be)\n",
    "   - note: to get sklearn perform the same algorithm as out implementation it expects you to provide non one-hot encoding labels. That's why we kept them in the first exercise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh = KNeighborsClassifier(n_neighbors=k)\n",
    "neigh.fit(X_train, y_train_not_onehot)\n",
    "y_pred_sklearn = neigh.predict_proba(X_test)\n",
    "\n",
    "y_pred_one_hot_sk = [one_hot_maker(y) for y in y_pred_sklearn]\n",
    "y_pred_not_onehot_sk = [y.index(max(y))+1 for y in y_pred_one_hot_sk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7009345794392523"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(np.array(y_test_not_onehot), np.array(y_pred_not_onehot_sk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[27,  7,  1,  0,  0,  0],\n",
       "       [ 5, 31,  1,  0,  1,  0],\n",
       "       [ 6,  0,  2,  0,  0,  0],\n",
       "       [ 0,  5,  0,  2,  0,  0],\n",
       "       [ 1,  1,  0,  0,  2,  0],\n",
       "       [ 1,  2,  1,  0,  0, 11]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test_not_onehot,y_pred_not_onehot_sk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f519ac30690>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sn.heatmap(confusion_matrix(y_test_not_onehot,y_pred_not_onehot_sk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_one_hot == y_pred_one_hot_sk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conlusion 5:\n",
    "Our algorithm gives the same prediction as the built in sklearn function. However there are some incorrectness in the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
